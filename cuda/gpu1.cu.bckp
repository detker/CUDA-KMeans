#include "gpu1.cuh"

template<int D>
__global__ void compute_clusters(const double* datapoints,
    int N, int K,
    int* assignments, unsigned int* assignmentsChanged, double* newClusters, int* clustersSizes, double *centroids_old)
{
    extern __shared__ char sharedMem[];

    unsigned int* changedFlag = (unsigned int*)sharedMem;
    double* clusters = (double*)(sharedMem + sizeof(unsigned int) * 32);

    if (threadIdx.x < K)
    {
        #pragma unroll
        for(int d=0; d < D; ++d)
        {
            clusters[d*K + threadIdx.x] = centroids_old[d * K + threadIdx.x];
        }

        if(clustersSizes[threadIdx.x] > 0) {
            #pragma unroll
            for(int d=0; d < D; ++d)
            {
                double val = newClusters[d * K + threadIdx.x] / clustersSizes[threadIdx.x];
                centroids_old[d * K + threadIdx.x] = val;
                clusters[d*K + threadIdx.x] = val;
            }
        }        
        // else {
        //     #pragma unroll
        //     for(int d=0; d < D; ++d)
        //     {
        //         clusters[d*K + threadIdx.x] = centroids_old[d * K + threadIdx.x];
        //     }
        // }
    }
    
    __syncthreads();

    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    unsigned int changed = 0;

    if (idx < N)
    {
        double minDistance = DEVICE_INF;
        int bestCluster = 0;

        for (int k = 0; k < K; k++) {
            double sum = 0.0;
            #pragma unroll
            for(int d=0; d < D; ++d)
            {
                double diff = datapoints[d * N + idx] - clusters[d * K + k];
                sum += diff * diff;
            }
            if (sum < minDistance) {
                minDistance = sum;
                bestCluster = k;
            }
        }

        changed = (assignments[idx] != bestCluster) ? 1 : 0;
        assignments[idx] = bestCluster; 
    }

    // Warp shuffle reduction
    for(int offset = 16; offset > 0; offset >>= 1)
    {
        changed += __shfl_down_sync(0xffffffff, changed, offset);
    }

    int lane_id = threadIdx.x & 31;
    int warp_id = threadIdx.x >> 5;

    if(lane_id == 0)
    {
        changedFlag[warp_id] = changed;
    }

    __syncthreads();

    if(warp_id == 0)
    {
        unsigned int warpSum = (threadIdx.x < (blockDim.x + 31) / 32) ? changedFlag[threadIdx.x] : 0;
        for(int offset = 16; offset > 0; offset >>= 1)
        {
            warpSum += __shfl_down_sync(0xffffffff, warpSum, offset);
        }

        if(threadIdx.x == 0)
        {
            atomicAdd(assignmentsChanged, warpSum);
        }
    }
}

template<int D>
__global__ void scatter_clusters(const double* datapoints, const int* assignments,
    int N, int K,
    double* newClusters, int* clustersSizes)
{
    int blocksPerCluster = gridDim.x / K;
    int k = blockIdx.x / blocksPerCluster;
    int blockId = blockIdx.x % blocksPerCluster;

    extern __shared__ double shmm[];
    double *localSums = shmm;
    unsigned int *localSizes = (unsigned int *)(shmm + blockDim.x * D);
    
    #pragma unroll
    for(int d=0; d < D; ++d)
    {
        localSums[d*blockDim.x + threadIdx.x] = 0.0;
    }
    localSizes[threadIdx.x] = 0;

    __syncthreads();

    int pointsPerBlock = (N + blocksPerCluster - 1) / blocksPerCluster;
    int startIdx = blockId * pointsPerBlock;
    int endIdx = min(startIdx + pointsPerBlock, N);

    for(int idx = startIdx + threadIdx.x; idx < endIdx; idx += blockDim.x)
    {
        if(assignments[idx] == k)
        {
            #pragma unroll
            for(int d=0; d < D; ++d)
            {
                localSums[d*blockDim.x + threadIdx.x] += datapoints[d*N + idx];
            }
            ++localSizes[threadIdx.x];
        }
    }

    __syncthreads();

    // Reduce within block
    for(int offset = blockDim.x / 2; offset > 0; offset >>= 1)
    {
        if(threadIdx.x < offset)
        {
            #pragma unroll
            for(int d=0; d < D; ++d)
            {
                localSums[d*blockDim.x + threadIdx.x] += localSums[d*blockDim.x + threadIdx.x+offset];
            }
            localSizes[threadIdx.x] += localSizes[threadIdx.x + offset];
        }
        __syncthreads();
    }

    if(threadIdx.x == 0)
    {
        #pragma unroll
        for(int d=0; d < D; ++d)
        {
            atomicAdd(&newClusters[d*K + k], localSums[d*blockDim.x + 0]);
        }
        atomicAdd(&clustersSizes[k], localSizes[0]);
    }
}

template<int D>
void kmeans_host(const double* datapoints, double* centroids,
    int N, int K, int* assignments, TimerManager *tm)
{
    TimerGPU timerGPU;
    tm->SetTimer(&timerGPU);

    size_t datapointsSize = N * D * sizeof(double);
    size_t centroidsSize = K * D * sizeof(double);
    size_t assignmentsSize = N * sizeof(int);
    size_t clustersSizesSize = K * sizeof(int);

    const double *deviceDatapoints;
    double *newClustersDevice;
    int *clustersSizesDevice;
    double *centroidsDevice_old;
    int* deviceAssignments;
    unsigned int* deviceDelta;

    CUDA_CHECK(cudaMalloc((void**)&deviceDatapoints, datapointsSize));
    CUDA_CHECK(cudaMalloc((void**)&newClustersDevice, centroidsSize));
    CUDA_CHECK(cudaMalloc((void**)&clustersSizesDevice, clustersSizesSize));
    CUDA_CHECK(cudaMalloc((void**)&centroidsDevice_old, centroidsSize));
    CUDA_CHECK(cudaMalloc((void**)&deviceAssignments, assignmentsSize));
    CUDA_CHECK(cudaMalloc((void**)&deviceDelta, sizeof(unsigned int)));

    CUDA_CHECK(cudaMemcpy((void*)deviceDatapoints, (const void*)datapoints, datapointsSize, cudaMemcpyHostToDevice));

    // Convert AoS to SoA for initial centroids
    for (int k = 0; k < K; ++k)
    {
        #pragma unroll
        for (int d = 0; d < D; ++d)
        {
            centroids[d * K + k] = datapoints[d * N + k];
        }
    }

    CUDA_CHECK(cudaMemcpy((void*)centroidsDevice_old, (const void*)centroids, centroidsSize, cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaMemcpy((void*)newClustersDevice, (const void*)centroids, centroidsSize, cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaMemset((void*)deviceAssignments, 0, assignmentsSize));
    CUDA_CHECK(cudaMemset((void*)clustersSizesDevice, 0, clustersSizesSize));

    unsigned delta = N;
    int threadsPerBlock = 256;
    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;
    int sharedMemSize = sizeof(unsigned int) * 32 + sizeof(double) * K * D;

    for (int iter = 0; iter < MAX_ITERATIONS && delta > 0; iter++)
    {
        CUDA_CHECK(cudaMemset((void*)deviceDelta, 0, sizeof(unsigned int)));

        tm->Start();
        compute_clusters<D><<<blocksPerGrid, threadsPerBlock, sharedMemSize>>>(
            deviceDatapoints, N, K, deviceAssignments, deviceDelta, 
            newClustersDevice, clustersSizesDevice, centroidsDevice_old);
        tm->Stop();
        CUDA_CHECK(cudaDeviceSynchronize());
        CUDA_CHECK(cudaGetLastError());

        CUDA_CHECK(cudaMemcpy((void*)&delta, (const void*)deviceDelta, sizeof(unsigned int), cudaMemcpyDeviceToHost));

        CUDA_CHECK(cudaMemset((void*)newClustersDevice, 0, centroidsSize));
        CUDA_CHECK(cudaMemset((void*)clustersSizesDevice, 0, clustersSizesSize));

        int numThreads = 256;
        int numBlocks = (N + numThreads - 1) / numThreads;
        int blocksPerCluster = (numBlocks + K - 1) / K;
        int totalBlocks = blocksPerCluster * K;
        int sharedMemSizeScatter = sizeof(double) * D * numThreads + sizeof(unsigned int) * numThreads;
        
        tm->Start();
        scatter_clusters<D><<<totalBlocks, numThreads, sharedMemSizeScatter>>>(
            deviceDatapoints, deviceAssignments, N, K, newClustersDevice, clustersSizesDevice);
        tm->Stop();
        CUDA_CHECK(cudaDeviceSynchronize());
        CUDA_CHECK(cudaGetLastError());

        printf("Iteration: %d, changes: %d\n", iter, delta);
    }

    CUDA_CHECK(cudaMemcpy((void*)assignments, (const void*)deviceAssignments, assignmentsSize, cudaMemcpyDeviceToHost));

    double* centroids_sum_col_major = (double*)malloc(centroidsSize);
    if (!centroids_sum_col_major) ERR("malloc centroids_sum_col_major failed.");
    int* centroids_count_col_major = (int*)malloc(clustersSizesSize);
    if (!centroids_count_col_major) ERR("malloc centroids_count_col_major failed.");
    double *centroids_old_col_major = (double*)malloc(centroidsSize);
    if (!centroids_old_col_major) ERR("malloc centroids_old_col_major failed.");

    CUDA_CHECK(cudaMemcpy((void*)centroids_sum_col_major, (const void*)newClustersDevice, centroidsSize, cudaMemcpyDeviceToHost));
    CUDA_CHECK(cudaMemcpy((void*)centroids_count_col_major, (const void*)clustersSizesDevice, clustersSizesSize, cudaMemcpyDeviceToHost));
    CUDA_CHECK(cudaMemcpy((void*)centroids_old_col_major, (const void*)centroidsDevice_old, centroidsSize, cudaMemcpyDeviceToHost));

    // Convert from column-major (SoA) to row-major (AoS) for output
    for (int k = 0; k < K; ++k)
    {
        if(centroids_count_col_major[k] > 0)
        {
            #pragma unroll
            for (int d = 0; d < D; ++d)
            {
                centroids[k * D + d] = centroids_sum_col_major[d * K + k] / centroids_count_col_major[k];
            }
        }
        else
        {
            #pragma unroll
            for (int d = 0; d < D; ++d)
            {
                centroids[k * D + d] = centroids_old_col_major[d * K + k];
            }
        }
    }

    CUDA_CHECK(cudaFree((void*)deviceDatapoints));
    CUDA_CHECK(cudaFree((void*)deviceAssignments));
    CUDA_CHECK(cudaFree((void*)newClustersDevice));
    CUDA_CHECK(cudaFree((void*)clustersSizesDevice));
    CUDA_CHECK(cudaFree((void*)deviceDelta));
    CUDA_CHECK(cudaFree((void*)centroidsDevice_old));
    
    free(centroids_old_col_major);
    free(centroids_count_col_major);
    free(centroids_sum_col_major);
}

using KMeansFunc = void(const double*, double*, int, int, int*, TimerManager*);

template KMeansFunc kmeans_host<1>;
template KMeansFunc kmeans_host<2>;
template KMeansFunc kmeans_host<3>;
template KMeansFunc kmeans_host<4>;
template KMeansFunc kmeans_host<5>;
template KMeansFunc kmeans_host<6>;
template KMeansFunc kmeans_host<7>;
template KMeansFunc kmeans_host<8>;
template KMeansFunc kmeans_host<9>;
template KMeansFunc kmeans_host<10>;
template KMeansFunc kmeans_host<11>;
template KMeansFunc kmeans_host<12>;
template KMeansFunc kmeans_host<13>;
template KMeansFunc kmeans_host<14>;
template KMeansFunc kmeans_host<15>;
template KMeansFunc kmeans_host<16>;
template KMeansFunc kmeans_host<17>;
template KMeansFunc kmeans_host<18>;
template KMeansFunc kmeans_host<19>;
template KMeansFunc kmeans_host<20>;