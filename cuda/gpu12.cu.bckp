#include "gpu1.cuh"

__device__ __constant__ double DEVICE_INF = DBL_MAX;

__host__ inline unsigned int nextPowerOfTwo(unsigned int n) 
{
    if (n == 0) return 1;
    
    n--;
    n |= n >> 1;
    n |= n >> 2;
    n |= n >> 4;
    n |= n >> 8;
    n |= n >> 16;
    n++;
    
    return n;
}

__host__ __device__ inline void compute_distance_l2(const double* points, const double* clusters, int idx, int k_idx, int N, int K, int D, double* result)
{
    double sum = 0.0;
    for(int d=0; d < D; ++d)
    {
        double diff = points[d * N + idx] - clusters[d * K + k_idx];
        sum += diff * diff;
    }
    *result = sum;

}

__global__ void transpose(const double* input, double* output, int N, int D)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < N)
    {
        #pragma unroll
        for(int d=0; d < D; ++d)
        {
            output[d * N + idx] = input[idx * D + d];
        }
    }
}

__global__ void compute_clusters(const double* datapoints, double *centroids,
    int N, int K, int D,
    int* assignments, unsigned int* assignmentsChanged, double* newClusters, int* clustersSizes)
{
    extern __shared__ char sharedMem[];

    unsigned int* changedFlag = (unsigned int*)sharedMem;

    double* clusters = (double*)(sharedMem + sizeof(unsigned int) * 32);

    if (threadIdx.x < K)
    {
        if (clustersSizes[threadIdx.x] > 0)
        {
            #pragma unroll
            for (int d = 0; d < D; ++d)
            {
                centroids[d*K + threadIdx.x] = newClusters[d*K + threadIdx.x] / clustersSizes[threadIdx.x];
            }
        }

        #pragma unroll
        for (int d = 0; d < D; ++d)
        {
            // clusters[d*K + threadIdx.x] = newClusters[d*K + threadIdx.x] / (clustersSizes[threadIdx.x] > 0 ? clustersSizes[threadIdx.x] : 1);
            clusters[d * K + threadIdx.x] = centroids[d*K + threadIdx.x];
        }
    }
    // changedFlag[threadIdx.x] = 0;
    __syncthreads();

    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    unsigned int changed = 0;

    if (idx < N)
    {
        double minDistance = DEVICE_INF;
        int bestCluster = 0;

        for (int k = 0; k < K; k++) {
            // double distance;
            // // compute_distance_l2(&datapoints[idx * D], &clusters[k * D], D, &distance);
            // compute_distance_l2(datapoints, clusters, idx, k, N, K, D, &distance);
            double sum = 0.0;
            #pragma unroll
            for(int d=0; d < D; ++d)
            {
                double diff = datapoints[d * N + idx] - clusters[d * K + k];
                sum += diff * diff;
            }
            // if (distance < minDistance) {
            if (sum < minDistance) {
                minDistance = sum;
                bestCluster = k;
            }
        }

        // if (assignments[idx] != bestCluster)
        // {
        //     assignments[idx] = bestCluster;
        //     changedFlag[threadIdx.x] = 1;
        // }
        changed = (assignments[idx] != bestCluster) ? 1 : 0;
        assignments[idx] = bestCluster; 
    }

    // warp shuffle
    for(int offset = 16; offset > 0; offset >>= 1)
    {
        changed += __shfl_down_sync(0xffffffff, changed, offset);
    }
    // lane 0 has the sum from warp

    int lane_id = threadIdx.x & 31; // same as threadIdx.x % 32
    int warp_id = threadIdx.x >> 5; // same as threadIdx.x / 32

    if(lane_id == 0)
    {
        changedFlag[warp_id] = changed;
    }

    __syncthreads();

    if(warp_id == 0)
    {
        unsigned int warpSum = (threadIdx.x < (blockDim.x + 31) / 32) ? changedFlag[threadIdx.x] : 0; // jesli mamy thread mniejszy niz liczba warpow w bloku to bierzemy wartosc z changedFlag
        for(int offset = 16; offset > 0; offset >>= 1)
        {
            warpSum += __shfl_down_sync(0xffffffff, warpSum, offset);
        }

        if(threadIdx.x == 0) // equivalent to if(threadIdx.x == 0)
        {
            // assignmentsChanged[blockIdx.x] = warpSum;
            atomicAdd(assignmentsChanged, warpSum);
        }
    }
}


// __global__ void compute_clusters(const double* datapoints, double *centroids,
//     int N, int K, int D,
//     int* assignments, unsigned int* assignmentsChanged, double* newClusters, int* clustersSizes)
// {
//     extern __shared__ char sharedMem[];

//     unsigned char* changedFlag = (unsigned char*)sharedMem;

//     double* clusters = (double*)(sharedMem + sizeof(unsigned char) * blockDim.x);

//     if (threadIdx.x < K)
//     {
//         if (clustersSizes[threadIdx.x] > 0)
//         {
//             #pragma unroll
//             for (int d = 0; d < D; ++d)
//             {
//                 centroids[d*K + threadIdx.x] = newClusters[d*K + threadIdx.x] / clustersSizes[threadIdx.x];
//             }
//         }

//         #pragma unroll
//         for (int d = 0; d < D; ++d)
//         {
//             // clusters[d*K + threadIdx.x] = newClusters[d*K + threadIdx.x] / (clustersSizes[threadIdx.x] > 0 ? clustersSizes[threadIdx.x] : 1);
//             clusters[d * K + threadIdx.x] = centroids[d*K + threadIdx.x];
//         }
//     }
//     changedFlag[threadIdx.x] = 0;
//     __syncthreads();

//     int idx = blockIdx.x * blockDim.x + threadIdx.x;
//     if (idx >= N)
//     {
//         // sharedMem[threadIdx.x] = 0;
//         return;
//     }

//     double minDistance = DEVICE_INF;
//     int bestCluster = 0;

//     for (int k = 0; k < K; k++) {
//         // double distance;
//         // // compute_distance_l2(&datapoints[idx * D], &clusters[k * D], D, &distance);
//         // compute_distance_l2(datapoints, clusters, idx, k, N, K, D, &distance);
//         double sum = 0.0;
//         #pragma unroll
//         for(int d=0; d < D; ++d)
//         {
//             double diff = datapoints[d * N + idx] - clusters[d * K + k];
//             sum += diff * diff;
//         }
//         // if (distance < minDistance) {
//         if (sum < minDistance) {
//             minDistance = sum;
//             bestCluster = k;
//         }
//     }

//     if (assignments[idx] != bestCluster)
//     {
//         assignments[idx] = bestCluster;
//         changedFlag[threadIdx.x] = 1;
//     }

//     __syncthreads();

//     for (int i = blockDim.x / 2; i > 0; i >>= 1)
//     {
//         if (threadIdx.x < i)
//         {
//             changedFlag[threadIdx.x] += changedFlag[threadIdx.x + i];
//         }
//         __syncthreads();
//     }

//     if (threadIdx.x == 0)
//     {
//         assignmentsChanged[blockIdx.x] = changedFlag[0];
//     }
// 	// sharedMem[threadIdx.x] = 0;
// }

__global__ void compute_delta(const unsigned int* assignmentsChanged, int N, unsigned int* delta)
{
    // tree based reduction here happens
    extern __shared__ unsigned int shm[];
    int tid = threadIdx.x;

    unsigned int sum = 0;
    for (int i = tid; i < N; i += blockDim.x)
    {
        sum += assignmentsChanged[i];
    }
    shm[tid] = sum;
    __syncthreads();

    if (tid >= N)
    {
        shm[tid] = 0;
        return;
    }

    for (int i = blockDim.x / 2; i > 0; i >>= 1)
    {
        if (tid < i)
        {
            shm[tid] += shm[tid + i];
        }
        __syncthreads();
    }

    if (tid == 0)
    {
        *delta = shm[0];
    }
    // shm[tid] = 0;
}

__global__ void scatter_clusters(const double* datapoints, const int* assignments,
    int N, int K, int D,
    double* newClusters, int* clustersSizes)
{
    int blocksPerCluster = gridDim.x / K; // number of blocks assigned to each cluster :)
    int k = blockIdx.x / blocksPerCluster; // [000000,111111,222222,...] cluster index
    int blockId = blockIdx.x % blocksPerCluster;  // block idx within cluster k

    extern __shared__ double shmm[];
    double *localSums = shmm;
    unsigned int *localSizes = (unsigned int *)(shmm + blockDim.x * D);
    #pragma unroll
    for(int d=0; d < D; ++d)
    {
        // localSums[D*threadIdx.x + d] = 0.0;
        localSums[d*blockDim.x + threadIdx.x] = 0.0;
    }
    localSizes[threadIdx.x] = 0;

    __syncthreads();

    // each block handles a chunk of data points
    int pointsPerBlock = (N + blocksPerCluster - 1) / blocksPerCluster;
    int startIdx = blockId * pointsPerBlock;
    int endIdx = min(startIdx + pointsPerBlock, N);

    for(int idx = startIdx + threadIdx.x; idx < endIdx; idx += blockDim.x)
    {
        if(assignments[idx] == k)
        {
            for(int d=0; d < D; ++d)
            {
                // localSums[D*threadIdx.x + d] += datapoints[D*idx + d];
                // localSums[D*threadIdx.x + d] += datapoints[d*N + idx];
                localSums[d*blockDim.x + threadIdx.x] += datapoints[d*N + idx];
            }
            ++localSizes[threadIdx.x];
        }
    }

    __syncthreads();

    // reduce within block
    for(int offset = blockDim.x / 2; offset > 0; offset >>= 1)
    {
        if(threadIdx.x < offset)
        {
            #pragma unroll
            for(int d=0; d < D; ++d)
            {
                // localSums[D*threadIdx.x + d] += localSums[D*(threadIdx.x + offset) + d];
                localSums[d*blockDim.x + threadIdx.x] += localSums[d*blockDim.x + threadIdx.x+offset];
            }
            localSizes[threadIdx.x] += localSizes[threadIdx.x + offset];
        }
        __syncthreads();
    }

    if(threadIdx.x == 0)
    {
        #pragma unroll
        for(int d=0; d < D; ++d)
        {
            // atomicAdd(&newClusters[D*k + d], localSums[d]);
            atomicAdd(&newClusters[d*K + k], localSums[d*blockDim.x + 0]);
        }
        atomicAdd(&clustersSizes[k], localSizes[0]);
    }
}

// __global__ void scatter_clusters(const double* datapoints, const int* assignments,
//     int N, int K, int D,
//     double* newClusters, int* clustersSizes)
// {
//     extern __shared__ char shmm[];

//     unsigned int *shmSizes = (unsigned int *)shmm;

//     size_t offset = ((sizeof(unsigned int) * K + 8 - 1) / 8) * 8; // align to double
//     double *shmClusters = (double*)(shmm + offset);

//     if(threadIdx.x < K)
//     {
//         shmSizes[threadIdx.x] = 0;
//         for (int d = 0; d < D; ++d)
//         {
//             // i think there are a lot of bank conflicts as we can have up to 20 stride (doesnt happen when e.g. D=4 K=5, test with larger vals, also check out for other places for bank conflicts)
//             shmClusters[d * K + threadIdx.x] = 0.0;
//         }
//     }

//     __syncthreads();

//     int idx = blockIdx.x * blockDim.x + threadIdx.x;
//     if (idx >= N) return;

//     int clusterIdx = assignments[idx];

//     for (int d = 0; d < D; ++d)
//     {
//         // atomicAdd(&shmClusters[clusterIdx * D + d], datapoints[idx * D + d]);
//         atomicAdd(&shmClusters[d * K + clusterIdx], datapoints[d * N + idx]);
//     }
//     atomicAdd(&shmSizes[clusterIdx], 1);


//     if(threadIdx.x < K)
//     {
//         __syncthreads(); // maybe put it inside if - since volta its clearly safe
//         for (int d = 0; d < D; ++d)
//         {
//             // atomicAdd(&newClusters[threadIdx.x * D + d], shmClusters[threadIdx.x * D + d]);
//             atomicAdd(&newClusters[d * K + threadIdx.x], shmClusters[d * K + threadIdx.x]);
//         }
//         atomicAdd(&clustersSizes[threadIdx.x], shmSizes[threadIdx.x]);
//     }
// }

extern "C"
void kmeans_host(const double* datapoints, double* centroids,
    int N, int K, int D, int* assignments, TimerManager *tm)
{
    


    TimerGPU timerGPU;
    tm->SetTimer(&timerGPU);

    // double *datapoints_col_major = (double*)malloc(N * D * sizeof(double));
    // if (!datapoints_col_major) ERR("malloc datapoints_col_major failed.");
    // row_to_col_major<double>(datapoints, datapoints_col_major, N, D); // AoS -> SoA

    const double *deviceDatapointsRowMajor;
    const double *deviceDatapoints;
    double* deviceCentroids;
    int* deviceAssignments;
    unsigned int* deviceAssignmentsChanged;
    double* newClusters;
    int* clustersSizes;
    size_t datapointsSize = N * D * sizeof(double);
    size_t centroidsSize = K * D * sizeof(double);
    size_t assignmentsSize = N * sizeof(int);
    size_t clustersSizesSize = K * sizeof(int);

    clustersSizes = (int*)malloc(clustersSizesSize);
    if (!clustersSizes) ERR("malloc clustersSizes failed.");
    memset((void*)clustersSizes, 0, clustersSizesSize);
    newClusters = (double*)malloc(centroidsSize);
    if (!newClusters) ERR("malloc newClusters failed.");
    memset((void*)newClusters, 0, centroidsSize);

    double *newClustersDevice;
    int *clustersSizesDevice;
    CUDA_CHECK(cudaMalloc((void**)&newClustersDevice, centroidsSize));
    CUDA_CHECK(cudaMalloc((void**)&clustersSizesDevice, clustersSizesSize));

    // CUDA_CHECK(cudaMalloc((void**)&deviceDatapointsRowMajor, datapointsSize));
    CUDA_CHECK(cudaMalloc((void**)&deviceDatapoints, datapointsSize));

    CUDA_CHECK(cudaMalloc((void**)&deviceCentroids, centroidsSize));
    CUDA_CHECK(cudaMalloc((void**)&deviceAssignments, assignmentsSize));

    // first we need to initialize centroids by first K datapoints (here we should ensure K <= N!)
    // TODO: add check for K <= N or simply memset centroids to 0 beforehand
    // CUDA_CHECK(cudaMemcpy((void*)deviceDatapointsRowMajor, (const void*)datapoints, datapointsSize, cudaMemcpyHostToDevice));
    // int colMajorThreadsPerBlock = 512;
    // int colMajorBlocksPerGrid = (N + colMajorThreadsPerBlock - 1) / colMajorThreadsPerBlock;
    // transpose<<<colMajorBlocksPerGrid, colMajorThreadsPerBlock>>>(deviceDatapointsRowMajor, (double *)deviceDatapoints, N, D);
    // CUDA_CHECK(cudaDeviceSynchronize());
    // CUDA_CHECK(cudaGetLastError());
    // CUDA_CHECK(cudaFree((void*)deviceDatapointsRowMajor));


    // CUDA_CHECK(cudaMemcpy((void*)deviceDatapoints, (const void*)datapoints_col_major, datapointsSize, cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaMemcpy((void*)deviceDatapoints, (const void*)datapoints, datapointsSize, cudaMemcpyHostToDevice));
    // CUDA_CHECK(cudaMemcpy((void*)deviceCentroids, (const void*)datapoints, centroidsSize, cudaMemcpyHostToDevice));

    // AoS -> SoA
    for (int k = 0; k < K; ++k)
    {
        for (int d = 0; d < D; ++d)
        {
            // centroids[d * K + k] = datapoints[k * D + d];
            centroids[d * K + k] = datapoints[d * N + k];
        }
    }

    CUDA_CHECK(cudaMemcpy((void*)deviceCentroids, (const void*)centroids, centroidsSize, cudaMemcpyHostToDevice));

    // Initialize assignments
    CUDA_CHECK(cudaMemset((void*)deviceAssignments, 0, assignmentsSize));


    CUDA_CHECK(cudaMemcpy((void*)newClustersDevice, (const void*)centroids, centroidsSize, cudaMemcpyHostToDevice));
    // CUDA_CHECK(cudaMemset((void*)newClustersDevice, 0, centroidsSize));
    CUDA_CHECK(cudaMemset((void*)clustersSizesDevice, 0, clustersSizesSize));

    unsigned delta = N;

    // Kernel launch parameters
    int threadsPerBlock = 512; // unsigned char limitation in shared memory usage
    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;
    // blocksPerGrid = nextPowerOfTwo(blocksPerGrid);
    // int sharedMemSize = sizeof(unsigned char) * threadsPerBlock + sizeof(double) * K * D;
    int sharedMemSize = sizeof(unsigned int) * 32 + sizeof(double) * K * D;
    
    int threadsPerBlockDelta = 512;
    int blocksPerGridDelta = 1;
    int sharedMemSizeDelta = sizeof(unsigned int) * threadsPerBlockDelta;

    CUDA_CHECK(cudaMalloc((void**)&deviceAssignmentsChanged, sizeof(unsigned int) * blocksPerGrid));
    CUDA_CHECK(cudaMemset((void*)deviceAssignmentsChanged, 0, sizeof(unsigned int) * blocksPerGrid));

    unsigned int* deviceDelta;
    CUDA_CHECK(cudaMalloc((void**)&deviceDelta, sizeof(unsigned int)));
    // CUDA_CHECK(cudaMemcpy((void*)deviceDelta, (const void*)&delta, sizeof(unsigned int), cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaMemset((void*)deviceDelta, 0, sizeof(unsigned int)));


    printf("GPU centroid 0: ");
    for (int d = 0; d < D; d++) {
        printf("%.10f ", centroids[d * K + 0]);
    }
    printf("\n");
    for (int iter = 0; iter < MAX_ITERATIONS && delta > 0; iter++)
    {
        // CUDA_CHECK(cudaMemcpy((void*)deviceCentroids, (const void*)centroids, centroidsSize, cudaMemcpyHostToDevice));
        // memset((void*)clustersSizes, 0, clustersSizesSize);
        
        tm->Start();
        compute_clusters << <blocksPerGrid, threadsPerBlock, sharedMemSize >> > (deviceDatapoints, deviceCentroids,
            N, K, D, deviceAssignments, deviceDelta, newClustersDevice, clustersSizesDevice);
        tm->Stop();
        CUDA_CHECK(cudaDeviceSynchronize());
        CUDA_CHECK(cudaGetLastError());

        CUDA_CHECK(cudaMemset((void*)newClustersDevice, 0, centroidsSize));
        CUDA_CHECK(cudaMemset((void*)clustersSizesDevice, 0, clustersSizesSize));

        // tm->Start();
        // compute_delta << <blocksPerGridDelta, threadsPerBlockDelta, sharedMemSizeDelta >> > (deviceAssignmentsChanged, blocksPerGrid, deviceDelta);
        // tm->Stop();
        // CUDA_CHECK(cudaDeviceSynchronize());
        // CUDA_CHECK(cudaGetLastError());

        CUDA_CHECK(cudaMemcpy((void*)&delta, (const void*)deviceDelta, sizeof(unsigned int), cudaMemcpyDeviceToHost));

        // Here we should recompute centroids based on new assignments
        // CUDA_CHECK(cudaMemset((void*)deviceAssignmentsChanged, 0, sizeof(unsigned int) * blocksPerGrid));
        CUDA_CHECK(cudaMemset((void*)deviceDelta, 0, sizeof(unsigned int)));
        // CUDA_CHECK(cudaMemcpy((void*)assignments, (const void*)deviceAssignments, assignmentsSize, cudaMemcpyDeviceToHost));

        // int start_idx = iter == 0 ? K : 0;
        // int numThreads = 128;
        int numThreads = 256;
        // int numBlocks = (N + numThreads - 1) / numThreads;
        // int sharedMemSizeScatter = ((sizeof(unsigned int) * K + 8 - 1) / 8) * 8 + sizeof(double) * K * D;
        
        int numBlocks = (N + numThreads - 1) / numThreads;
        int blocksPerCluster = (numBlocks + K - 1) / K;
        // int blo
        int totalBlocks = blocksPerCluster * K;
        // totalBlocks = nextPowerOfTwo(totalBlocks);
        // int totalBlocks = 4000;
        // int sharedMemSizeScatter = ((sizeof(unsigned int) * K + 8 - 1) / 8) * 8 + sizeof(double) * K * D;
        int sharedMemSizeScatter = sizeof(double) * D * numThreads + sizeof(unsigned int) * numThreads;
        // tm->SetTimer(&timerCPU);
        tm->Start();
        // scatter_clusters << <numBlocks, numThreads, sharedMemSizeScatter >> > (deviceDatapoints, deviceAssignments,
        //     N, K, D,
        //     newClustersDevice, clustersSizesDevice);
        scatter_clusters << <totalBlocks, numThreads, sharedMemSizeScatter >> > (deviceDatapoints, deviceAssignments,
            N, K, D,
            newClustersDevice, clustersSizesDevice);
        tm->Stop();
        CUDA_CHECK(cudaDeviceSynchronize());
        CUDA_CHECK(cudaGetLastError());

        printf("Iteration: %d, changes: %d\n", iter, delta);

    }



    CUDA_CHECK(cudaMemcpy((void*)assignments, (const void*)deviceAssignments, assignmentsSize, cudaMemcpyDeviceToHost));
    CUDA_CHECK(cudaMemcpy((void*)newClusters, (const void*)newClustersDevice, centroidsSize, cudaMemcpyDeviceToHost));
    CUDA_CHECK(cudaMemcpy((void*)clustersSizes, (const void*)clustersSizesDevice, clustersSizesSize, cudaMemcpyDeviceToHost));
    CUDA_CHECK(cudaMemcpy((void*)centroids, (const void*)deviceCentroids, centroidsSize, cudaMemcpyDeviceToHost));
    // AoS
    // for (int k = 0; k < K; ++k)
    // {
    //     for (int d = 0; d < D; ++d)
    //     {
    //         if (clustersSizes[k] > 0) centroids[k * D + d] = newClusters[k * D + d] / clustersSizes[k];
    //         else centroids[k * D + d] = 0.0;
    //         newClusters[k * D + d] = 0.0;
    //     }
    //     clustersSizes[k] = 0;
    // }
    // SoA
    for (int k = 0; k < K; ++k)
    {
        for (int d = 0; d < D; ++d)
        {
            if (clustersSizes[k] > 0) centroids[k * D + d] = newClusters[d * K + k] / clustersSizes[k];
            // else centroids[k * D + d] = 0.0;
            newClusters[d * K + k] = 0.0;
        }
        clustersSizes[k] = 0;
    }

    // Just copy and convert layout:
    // CUDA_CHECK(cudaMemcpy((void*)centroids, (const void*)deviceCentroids, centroidsSize, cudaMemcpyDeviceToHost));

    // // Convert from column-major to row-major
    // double* temp = (double*)malloc(centroidsSize);
    // memcpy(temp, centroids, centroidsSize);
    // for (int k = 0; k < K; ++k) {
    //     for (int d = 0; d < D; ++d) {
    //         centroids[k * D + d] = temp[d * K + k];
    //     }
    // }
    // free(temp);


    // if (D == 3) 
    // {
    //     float minx, maxx, miny, maxy, minz, maxz;
    //     compute_bounds(datapoints, N, minx, maxx, miny, maxy, minz, maxz);
    //     render(deviceDatapoints, deviceAssignments, N, K, minx, maxx, miny, maxy, minz, maxz);
    // }

    // CUDA_CHECK(cudaMemcpy((void*)assignments, (const void*)deviceAssignments, assignmentsSize, cudaMemcpyDeviceToHost));
    //CUDA_CHECK(cudaMemcpy((void*)centroids, (const void*)deviceCentroids, centroidsSize, cudaMemcpyDeviceToHost));
    CUDA_CHECK(cudaFree((void*)deviceDatapoints));
    CUDA_CHECK(cudaFree((void*)deviceCentroids));
    CUDA_CHECK(cudaFree((void*)deviceAssignments));
    CUDA_CHECK(cudaFree((void*)deviceAssignmentsChanged));
    CUDA_CHECK(cudaFree((void*)newClustersDevice));
    CUDA_CHECK(cudaFree((void*)clustersSizesDevice));
    CUDA_CHECK(cudaFree((void*)deviceDelta));
    // free(datapoints_col_major);
    free(clustersSizes);
    free(newClusters);
}
